#+TITLE: Apache Spark
#+FILETAGS: :Software:Data:
#+STARTUP: contents, hideallblocks

* Introduction
:PROPERTIES:
:ID:       c63978d7-17dd-49e9-a52f-2fb47c7190c3
:END:

Apache Spark is a unified computing engine and set of libraries for
parallel data processing on computing clusters.

Spark is unified in the sense that it can handle a variety of
computing tasks ranging from simple SQL queries to ML
computations. Further, it can optimize combinations of these
computations.

Spark differs from Apache Hadoop in that Hadoop specifies both a file
storage system, Hadoop Distributed File System (HDFS), and a computing
engine, MapReduce. In Apache Hadoop, these are tightly
integrated. Apache Spark, however, focuses entirely on the computing
engine part and keeps the storage as abstracted as possible.

Spark provides several libraries on top of its core engine. For
instance, it has libraries for SQL (Spark SQL), machine learning
(Spark MLLib), stream processing (Spark Streaming), and graph
analytics (graphX).

Collecting data in the real world keeps getting cheaper due to
improvements in camera technology, reduced prices of sensors,
etc. Storing that data also keeps getting cheaper. However, the
processing of data relies more and more on parallel computation rather
than increasing processor speeds. This is the driving motivation for
Spark.

* Transformations and Actions

Spark allows the user to specify a sequence of transformations to
data. This sequence of transformations forms a Directed Acyclic Graph
(DAG). Spark then executes an action which is the implementation of
that DAG. Spark does this efficiently by breaking down the DAG into
stages and tasks that can be run on a cluster of computers.

* DataFrames

DataFrames are table-like collections with rows and columns. Each
column must have the same number of rows although some elements can be
null. All entries in a column must be of the same type.

Spark also allows Datasets which check types against schemas at
compile time. However, this is only supported in JVM-based languages
like Scala and Java.

* Code

Start a local Spark session in Python.

#+begin_src python
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
#+end_src

* Tasks

** DONE Clean up Spark setup and ipython startup with Emacs editor
SCHEDULED: <2025-11-04 Tue>
:PROPERTIES:
:EFFORT:  00:15
:BENEFIT: 10
:RATIO: 0.40
:END:
- State "DONE"       from "TODO"       [2025-11-04 Tue 08:33]
:LOGBOOK:
CLOCK: [2025-11-04 Tue 07:53]--[2025-11-04 Tue 08:33] =>  0:40
:END:

** TODO Read Apache Spark book (50/500) (~500 pages at 10 pages/hour)
SCHEDULED: <2025-11-04 Tue +1d>
:PROPERTIES:
:EFFORT:  50:00
:BENEFIT: 2000
:RATIO: 0.40
:LAST_REPEAT: [2025-11-03 Mon 08:26]
:END:
- State "DONE"       from "TODO"       [2025-11-03 Mon 08:26]
- State "DONE"       from "TODO"       [2025-10-29 Wed 09:02]
- State "DONE"       from "TODO"       [2025-10-28 Tue 08:19]
:LOGBOOK:
CLOCK: [2025-11-03 Mon 07:55]--[2025-11-03 Mon 08:24] =>  0:29
CLOCK: [2025-10-29 Wed 07:26]--[2025-10-29 Wed 08:26] =>  1:00
CLOCK: [2025-10-28 Tue 07:11]--[2025-10-28 Tue 08:19] =>  1:08
:END:


** TODO Remove C:/MSys2/tmp/Spark installation and delete [[id:8f83b767-d1ad-4494-a08c-262b2911b798][doc section]] if everything is working smoothly
SCHEDULED: <2025-11-23 Sun>
:PROPERTIES:
:EFFORT:  00:15
:BENEFIT: 10
:RATIO: 0.40
:END:

** Archive                                                          :ARCHIVE:
*** DONE Get the pyspark cmd to work - may need an installation of Spark
:PROPERTIES:
:ARCHIVE_TIME: 2025-10-29 Wed 08:26
:END:
:LOGBOOK:
CLOCK: [2025-10-28 Tue 14:26]--[2025-10-28 Tue 14:56] =>  0:30
:END:

*** DONE Fix the issue with "\*.csv" read - may need winutils
SCHEDULED: <2025-10-30 Thu>
:PROPERTIES:
:EFFORT:  00:30
:BENEFIT: 20
:RATIO: 0.40
:ARCHIVE_TIME: 2025-11-04 Tue 08:33
:END:
:LOGBOOK:
CLOCK: [2025-11-03 Mon 06:29]--[2025-11-03 Mon 07:48] =>  1:19
:END:
[2025-11-03 Mon]

* References

- Bill Chambers and Matei Zaharia, Spark: The Definitive Guide
- [[https://github.com/databricks/Spark-The-Definitive-Guide][GitHub code for the book]]
- [[id:d2581d0c-3791-4e3d-b2f3-a61f2b32e324][Spark and PySpark installation on PC]]
