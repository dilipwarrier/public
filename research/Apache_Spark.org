#+TITLE: Data Analytics using Apache Spark and Pandas
#+FILETAGS: :Software:Data:Analytics:
#+STARTUP: contents, hideallblocks

* Introduction
:PROPERTIES:
:ID:       c63978d7-17dd-49e9-a52f-2fb47c7190c3
:END:

Apache Spark is a unified computing engine and set of libraries for
parallel data processing on computing clusters.

Spark is unified in the sense that it can handle a variety of
computing tasks ranging from simple SQL queries to ML
computations. Further, it can optimize combinations of these
computations.

Spark differs from Apache Hadoop in that Hadoop specifies both a file
storage system, Hadoop Distributed File System (HDFS), and a computing
engine, MapReduce. In Apache Hadoop, these are tightly
integrated. Apache Spark, however, focuses entirely on the computing
engine part and keeps the storage as abstracted as possible.

Spark provides several libraries on top of its core engine. For
instance, it has libraries for SQL (Spark SQL), machine learning
(Spark MLLib), stream processing (Spark Streaming), and graph
analytics (graphX).

Collecting data in the real world keeps getting cheaper due to
improvements in camera technology, reduced prices of sensors,
etc. Storing that data also keeps getting cheaper. However, the
processing of data relies more and more on parallel computation rather
than increasing processor speeds. This is the driving motivation for
Spark.

* Startup code

For all the following sections, first complete the following steps.
- Complete [[id:4ef5ba87-d94b-494d-af79-e4f95df7a55c][Python installation]]
- Complete [[id:d2581d0c-3791-4e3d-b2f3-a61f2b32e324][Spark and PySpark installation on PC]]

Then, start a local Spark session in Python using the following code.

#+BEGIN_SRC python
  from pyspark.sql import SparkSession
  import pyspark.sql.functions as sqlf

  spark = SparkSession.builder.master("local[*]").getOrCreate()

  spark_data_dir = "C:/Users/dilip/Documents/GitHub/Spark-The-Definitive-Guide/data/"
  json_file_name = spark_data_dir + "flight-data/json/2015-summary.json"
  csv_file_name = spark_data_dir + "retail-data/by-day/2010-12-01.csv"

  df1_spark = spark.read\
                   .format("json")\
                   .load(json_file_name)

  # The JSON file has malformed lines that Pandas fails on.
  # So, read the JSON file through Spark first and then convert it to Pandas
  df1_pd = df1_spark.toPandas()

  df2_spark = spark.read\
                   .option("header", "true")\
                   .format("csv")\
                   .option("inferSchema", "true")\
                   .load(csv_file_name)

  df2_pd = pd.read_csv(csv_file_name,\
                       header=0)

#+END_SRC

The default schema inference appears to be better for Spark compared
to Pandas. See the results below.

#+BEGIN_SRC python
  df2_spark.schema
  df2_pd.dtypes

  # Columns like InvoiceNo are set to StringType in Spark but as generic
  # objects in Pandas
#+END_SRC

* Transformations and actions

Spark allows the user to specify a sequence of transformations to
data. This sequence of transformations forms a Directed Acyclic Graph
(DAG). Spark then executes an action which is the implementation of
that DAG. Spark does this efficiently by breaking down the DAG into
stages and tasks that can be run on a cluster of computers.

* DataFrames

DataFrames are table-like collections with rows and columns.

** Rows and columns

A row in a DataFrame is a single record. Spark represents it as an
object of type Row.

#+BEGIN_SRC python
  from pyspark.sql import Row

  myRow = Row("Hello", None, 1, False)
  print(myRow[0])
#+END_SRC

A column represents a value computed on a per-record
basis using an expression or operation. Thus, to have a value for a
column, you need a row and, to have a row, you need a DataFrame.

#+BEGIN_SRC python
  df1_spark.columns

  # Refer to a column name directly
  df1_spark.DEST_COUNTRY_NAME
#+END_SRC

Every DataFrame has an associated schema that maintains the
meaning of each record and associated data type.

#+BEGIN_SRC python
  df1_spark.printSchema()
#+END_SRC

** Selecting and expressing

The simplest and most common operation is to select records based on
column names. This is the DataFrame equivalent of an SQL query.

#+BEGIN_SRC python
  df1_spark.select("DEST_COUNTRY_NAME").show(5)

  # This is equivalent to the following command in SQL
  # SELECT DEST_COUNTRY_NAME from df LIMIT 5

  # You can also run direct SQL queries by converting df1_spark into
  # a local SQL table using createOrReplaceTempView()
  df1_spark.createOrReplaceTempView("df1_spark")
  spark.sql("SELECT DEST_COUNTRY_NAME from df1_spark LIMIT 5").show()

  df1_pd.DEST_COUNTRY_NAME[0:4]
#+END_SRC

An expression is a SQL-like query that takes as
inputs column names and does transformations on them.

#+BEGIN_SRC python
  df1_spark.select(sqlf.expr("DEST_COUNTRY_NAME as destination")).show(2)
#+END_SRC

select(expr()) can be compressed to selectExpr().

** Filtering

You can filter records using filter() or, equivalently, where().

#+BEGIN_SRC python
  df1_spark.filter(df1_spark.DEST_COUNTRY_NAME == "United States").show(2)

  df1_pd[df1_pd.DEST_COUNTRY_NAME == "United States"]
#+END_SRC

* Aggregations

Aggregations are types of data transformations that collect rows of a
DataFrame together and summarize them. Most aggregations are available
in the spark.sql.functions package.

The simplest aggregation is a count. Other examples are sum, min/max,
average, etc.

#+BEGIN_SRC python
  df2_spark.count()

  df2_pd.count()
#+END_SRC

Often, you need to group records using the values of certain columns
first. You can use the groupBy() method to do this. It results in a
GroupedData object.

The agg() method can be used to aggregate GroupedData objects using
a variety of aggregation transformations like count, sum, etc. The output of
the agg() method is a DataFrame.

#+BEGIN_SRC python
  df2_group = df2_spark.groupBy("InvoiceNo")
  df2_group.agg(sqlf.count("Quantity")).sort("InvoiceNo").show()

  df2_pd.groupby("InvoiceNo").Quantity.count()
#+END_SRC

Spark also provides the ability to aggregate with a
tolerance for error. This can be useful to speed up computations.

* Joins

Joins allow you to combine dataframes. Inner joins keep rows with keys
that exist in both the left and right dataframes. Outer joins keep rows
with keys that exist in either.

#+BEGIN_SRC python
  d1 = {"Person": ["John", "Myla", "Lewis"],
        "Age": [24, 22, 21],
        "Single": [False, True, True]}

  d2 = {"Age": [21, 22, 24],
        "Group": ["Z", "B", "C"]}

  d1_pd = pd.DataFrame(d1)
  d2_pd = pd.DataFrame(d2)

  d1_spark = spark.createDataFrame(d1_pd)
  d2_spark = spark.createDataFrame(d2_pd)

  # Join the data frames in Pandas using Age
  d1_pd.set_index('Age').\
      join(d2_pd.set_index('Age'), how = "inner")

  # Join the data frames in Spark using Age
  d1_spark.join(d2_spark, d1_spark["Age"] == d2_spark["Age"], "inner").show()
#+END_SRC

* Comparison among data analytics packages

** Spark vs Pandas

Both Spark and Pandas can be used for data analysis and
processing. Typically, Pandas operates in the MB-GB range of data sizes
while Spark operates in the TB-EB range. Further, Pandas runs locally
on your PC while Spark is optimized to run in a distributed fashion on
computing clusters.

A typical workflow might be to use Spark to process and clean a large
data set but then use Pandas to sample a small fraction of it for
exploration. Since both use data frames as their base types, it is
easy to convert between the two.

** NumPy vs Pandas

NumPy allows you to do fast mathematical operations on numeric
multi-dimensional arrays e.g. FFTs, linear algebra, etc. Pandas, on
the other hand, allows you to manipulate 2-D arrays with heterogeneous
data types.

A typical workflow might be to use Pandas to clean and pre-process
structured data into numeric data and then use NumPy to do analysis on
the numeric data efficiently.

Under the hood, Pandas uses NumPy arrays.

* References

- Bill Chambers and Matei Zaharia, Spark: The Definitive Guide
- [[https://github.com/databricks/Spark-The-Definitive-Guide][GitHub code for the Spark book]]
- [[https://spark.apache.org/docs/latest/api/python/user_guide/][PySpark User Guide]]
- [[https://pandas.pydata.org/docs/reference/index.html][Pandas API reference]]

* Tasks

** TODO Read Apache Spark book (152/500) (~500 pages at 10 pages/hour) :Career:
SCHEDULED: <2025-11-18 Tue +1d>
:PROPERTIES:
:EFFORT:  50:00
:BENEFIT: 2000
:RATIO: 0.40
:LAST_REPEAT: [2025-11-17 Mon 08:23]
:END:
- State "DONE"       from "TODO"       [2025-11-17 Mon 08:23]
- State "DONE"       from "TODO"       [2025-11-12 Wed 07:53]
- State "DONE"       from "TODO"       [2025-11-11 Tue 07:59]
- State "DONE"       from "TODO"       [2025-11-07 Fri 16:33]
- State "DONE"       from "TODO"       [2025-11-07 Fri 16:33]
- State "DONE"       from "TODO"       [2025-11-07 Fri 16:32]
- State "DONE"       from "TODO"       [2025-11-07 Fri 09:01]
- State "DONE"       from "TODO"       [2025-11-06 Thu 08:26]
- State "DONE"       from "TODO"       [2025-11-06 Thu 08:26]
- State "DONE"       from "TODO"       [2025-11-05 Wed 08:01]
- State "DONE"       from "TODO"       [2025-11-03 Mon 08:26]
- State "DONE"       from "TODO"       [2025-10-29 Wed 09:02]
- State "DONE"       from "TODO"       [2025-10-28 Tue 08:19]
:LOGBOOK:
CLOCK: [2025-11-17 Mon 06:33]--[2025-11-17 Mon 08:23] =>  1:50
CLOCK: [2025-11-12 Wed 06:24]--[2025-11-12 Wed 07:48] =>  1:24
CLOCK: [2025-11-11 Tue 06:29]--[2025-11-11 Tue 07:59] =>  1:30
CLOCK: [2025-11-07 Fri 06:36]--[2025-11-07 Fri 09:01] =>  2:25
CLOCK: [2025-11-06 Thu 06:35]--[2025-11-06 Thu 08:25] =>  1:50
CLOCK: [2025-11-05 Wed 06:54]--[2025-11-05 Wed 07:49] =>  0:55
CLOCK: [2025-11-03 Mon 07:55]--[2025-11-03 Mon 08:24] =>  0:29
CLOCK: [2025-10-29 Wed 07:26]--[2025-10-29 Wed 08:26] =>  1:00
CLOCK: [2025-10-28 Tue 07:11]--[2025-10-28 Tue 08:19] =>  1:08
:END:


** TODO Remove C:/MSys2/tmp/Spark installation and delete [[id:8f83b767-d1ad-4494-a08c-262b2911b798][doc section]] if everything is working smoothly
SCHEDULED: <2025-11-23 Sun>
:PROPERTIES:
:EFFORT:  00:15
:BENEFIT: 10
:RATIO: 0.40
:END:

** Archive                                                          :ARCHIVE:
*** DONE Get the pyspark cmd to work - may need an installation of Spark
:PROPERTIES:
:ARCHIVE_TIME: 2025-10-29 Wed 08:26
:END:
:LOGBOOK:
CLOCK: [2025-10-28 Tue 14:26]--[2025-10-28 Tue 14:56] =>  0:30
:END:

*** DONE Fix the issue with "\*.csv" read - may need winutils
SCHEDULED: <2025-10-30 Thu>
:PROPERTIES:
:EFFORT:  00:30
:BENEFIT: 20
:RATIO: 0.40
:ARCHIVE_TIME: 2025-11-04 Tue 08:33
:END:
:LOGBOOK:
CLOCK: [2025-11-03 Mon 06:29]--[2025-11-03 Mon 07:48] =>  1:19
:END:
[2025-11-03 Mon]

*** DONE Clean up Spark setup and ipython startup with Emacs editor
SCHEDULED: <2025-11-04 Tue>
:PROPERTIES:
:EFFORT:  00:15
:BENEFIT: 10
:RATIO: 0.40
:ARCHIVE_TIME: 2025-11-09 Sun 09:48
:END:
- State "DONE"       from "TODO"       [2025-11-04 Tue 08:33]
:LOGBOOK:
CLOCK: [2025-11-04 Tue 07:53]--[2025-11-04 Tue 08:33] =>  0:40
:END:

*** DONE Fix warning messages at Spark initialization (see below)
:PROPERTIES:
:EFFORT:  00:15
:BENEFIT: 10
:RATIO: 0.40
:ARCHIVE_TIME: 2025-11-16 Sun 09:12
:END:

- State "DONE"       from "TODO"       [2025-11-12 Wed 20:50]
#+BEGIN_QUOTE
WARNING: Using incubator modules: jdk.incubator.vector
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/11/05 07:58:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
#+END_QUOTE
