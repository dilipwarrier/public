#+TITLE: Interesting facts/news in neural networks/AI
#+FILETAGS: :AI:

* Generative AI                                                      :Review:

  GPT = Generative Pretrained Transformer

  It takes a string of words and rolls a die to predict the next
  word. It does this sequentially until a stopping condition is met.

  GPT-3 was generated using nearly 500B tokens of data. A token is
  roughly a word.

  GPT-3 has nearly 175B parameters.

  This [[https://drive.google.com/file/d/1P0chJKuHdGFL_Pshl6l0wyqMJ4Gu_KQV/view?usp=drivesdk][article]] describes details of how GPT works.

  GPT-3 was trained on several sources of data, but the bulk of it comes
  from snapshots of the entire internet between 2016 and 2019 taken from
  a database called Common Crawl. There’s a lot of junk text on the
  internet, so the initial 45 terabytes were filtered using a different
  machine-learning model to select just the high-quality text: 570
  gigabytes of it, a dataset that could fit on a modern laptop. In
  addition, GPT-4 was trained on an unknown quantity of images, probably
  several terabytes.

  The inputs of LLMs — data, computing power, electricity, and skilled
  labour — cost money. Training GPT-3, for example, used 1.3
  gigawatt-hours of electricity (enough to power 121 homes in America
  for a year), and cost OpenAI an estimated $4.6M. GPT-4, which is a
  much larger model, will have cost disproportionately more (in the
  realm of $100M) to train.


** Solving math problems

   According to this [[https://www.wsj.com/articles/chatgpt-openai-math-artificial-intelligence-8aba83f0][WSJ article]], GPT-3.5 and GPT-4 have gotten worse in
   August 2023 compared to March of the same year at solving math problems.
