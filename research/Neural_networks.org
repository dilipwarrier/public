#+TITLE: Interesting facts/news in neural networks/AI
#+FILETAGS: :AI:

GPT-3 was trained on several sources of data, but the bulk of it comes
from snapshots of the entire internet between 2016 and 2019 taken from
a database called Common Crawl. There’s a lot of junk text on the
internet, so the initial 45 terabytes were filtered using a different
machine-learning model to select just the high-quality text: 570
gigabytes of it, a dataset that could fit on a modern laptop. In
addition, GPT-4 was trained on an unknown quantity of images, probably
several terabytes.

The inputs of LLMs — data, computing power, electricity, and skilled
labour — cost money. Training GPT-3, for example, used 1.3
gigawatt-hours of electricity (enough to power 121 homes in America
for a year), and cost OpenAI an estimated $4.6M. GPT-4, which is a
much larger model, will have cost disproportionately more (in the
realm of $100M) to train.
