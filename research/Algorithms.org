#+TITLE: Algorithms to live by
#+Author: Brian Christian, Tom Griffiths
#+Filetags: :Reading:

** Optimal stopping

   The secretary problem was first presented by Martin Gardner in
   Scientific American in a 1960s puzzle.

   Assume you have a position for a secretary available. Assume further
   that, if you can make an offer to a candidate, the candidate will
   accept and that you can not go back and make an offer to a candidate
   whom you rejected earlier. What is the optimal strategy you should
   pursue? There is a theoretical answer: you should look at 37% of
   candidates and, after that, make an offer to any candidate who is
   better than the best candidate in that first 37%. Interestingly, the
   probability of finding the best candidate with this strategy is also
   37%.

   In real life, there is cost associated with looking for candidates. If
   you factor in reasonable costs, the number reduces from 37% to about
   30%. In experiments, human beings seem to use this number to solve
   such problems.

   If you are allowed to go back and make offers to candidates you
   previously rejected (and assume that those candidates will accept),
   then you can look for up to 60% of the time.


** Explore vs exploit

   The [[https://en.wikipedia.org/wiki/Multi-armed_bandit][multi-armed bandit]] problem is a problem where you have an
   number of slot machines to play. Each of them have an
   unknown but fixed probability of success with equal payoffs for any
   slot machine when it is successful. What is your best strategy to
   play the machines?

   A well-known solution to this problem uses the [[https://en.wikipedia.org/wiki/Gittins_index][Gittins index]]. The
   Gittins index considers geometric weighting for future rewards. It
   gives a 2-D matrix with columns representing the number of
   successful attempts so far and rows representing the unsuccessful
   ones. Each entry in the matrix is the Gittins index for that pair
   of successful and unsuccessful attempts so far. In your next move,
   you should play the machine with the highest Gittins index.

   The Gittins index is actually 0.7 even when you have not made any
   attempts on a machine (as opposed to 0.5). This is because when you
   haven't played a machine yet, there is some probability that it
   will turn out to be a high-paying machine. The future rewards of
   that machine can be quite high if it is high-paying and can be
   limited if it is low-paying because you will use the Gittins index
   to switch to some other machine if that is the case.

   For non-geometric future payoffs, there are other approaches like
   Upper Confidence Level optimization (choose a scenario that has a
   high upper threshold even if the lower threshold is low).

   Gittins index is also optimal if the state of the played machine
   changes over time. The version of the problem where the state of
   the *unplayed* machines change i.e. their payoff reward or the
   probability of payoff changes over time is called the restless
   bandit problem. That problem is known to be NP-complete. The
   Gittins index is expected to be a good heuristic for that
   problem. The [[http://www.anthonybonifonte.com/wp-content/uploads/2014/08/RMAB-Report-Final-AB-QC.pdf][Whittle index]] is supposed to be an optimal solution
   but is hard to compute. In general, the state space explodes pretty
   quickly for such problems.

   This problem has an interesting business application: how do you
   determine how to apply fixed resources to a limited set of
   projects, each of which have a probability of success? The
   real-life challenge to model this as a MAB problem is how you will
   measure success in exploratory R&D work.


*** Practical computations of Gittins index

    [[https://sites.google.com/site/lorenzodigregorio/gittins-index][This site]] provides Matlab code for the computation.

    [[https://arxiv.org/pdf/1909.05075v1.pdf][This paper]] and the associated Github site provides an R package
    for computation of the index for Bernoulli distributions.


*** Upper bound on Gittins index

   According to [[http://www.ece.mcgill.ca/~amahaj1/projects/bandits/book/2013-bandit-computations.pdf][Mahajan et al]], the following is an upper bound on the
   Gittins index.
   $\mu + \sigma \sqrt\frac{\beta}{1 - \beta}$

   where:
      - $\mu = p/(p + q)$
      - $\sigma^2 = \frac{p \times q}{(p + q)^2 \times (p + q + 1)}$
      - $\beta$ is the forgetting factor
      - $p$ and $q$ are the numbers of successful and unsuccessful
        attempts so far

   For $\beta = 0.9$, $\mu = 0.5$ and $p=q=0$, you get an upper bound
   of 2.0 which is quite loose compared to the value of 0.7 mentioned
   above. This bound is probably better for large values of $p+q$.


** Sorting

   Bubble sort is a somewhat inefficient way of sorting a
   list. MergeSort is a roughly O(n*logn) way of sorting. MergeSort
   requires you to start with 2 elements at a time, sort them, then
   combine with another sorted list of 2 elements, sort the combined
   list etc. and build your way up. In this way, it is quite similar
   to the Cooley-Tukey FFT algorithm and has the same O(n*logn)
   property.

   However, one can ask the meta-question of why sorting is
   needed. For many practical problems, you care about accessing
   information. In such situations, it is observed that using a Most
   Recently Used (MRU) approach is better because, if you have
   recently used something, it is likely that you will use it
   again. This beats approaches like FIFO or Most Recently Added (MRA).

   Libraries use MRA approaches when they put new books in shelves up
   front. Instead, the MRU approach would suggest that the most
   popular books should be up front, thus reducing librarian work to
   shelve such books and user effort to find them.


** Scheduling
   :PROPERTIES:
   :CUSTOM_ID: sched_alg
   :END:

   If you want to optimize the weighted sum of all processing times,
   then ranking tasks by the benefit divided by the remaining
   processing time is quite good.

   It can also be used to handle pre-emption where you do the same
   calculation for each incoming task. If the incoming task has a
   better benefit per unit time, you switch to that task.


** Probabilities

   A common problem is how to predict the value of a variable given
   what you know of its current state. For instance, if you are 50
   years old now, roughly how long will you live?

   This problem can be solved by applying knowledge of the underlying
   probability distribution to Bayes' theorem.

   There are three widely-observed probability distributions
   - Gaussian or normal
   - Power law
   - Erlang

  People's average lifetimes are represented well by Gaussian
  distributions. In this case, the best predictor is the average
  value. If you know the current age of a person, then you should
  predict roughly the average lifetime if their current age is well
  below it and add a little bit over the average if their age is above
  it.

  Gross incomes for movies are well represented by power laws. In this case,
  the best predictor is a multiplier factor. For no further knowledge,
  the best multiplier is 2. For movie incomes, it is known to be about
  1.4. Thus, if a movie has made $5M
  so far, your best predictor is that the movie will make $7M
  lifetime. If it has made $50M so far, the best prediction is that it
  will make $70M in its lifetime. Note how this multiplier factor is
  different from the average factor for Gaussian distributions.

  The number of years a politician holds office follows Erlangian
  distributions. In this case, the best predictor is an additive
  factor, irrespective of how long the politician has held office.

*** The marshmallow test

    Walter Mischel's famous "marshmallow test" consisted of an adult
    leaving one marshmallow on the table for a child, promising that if
    the child didn't eat the marshmallow by the time the adult came
    back, he or she would get two marshmallows, and then walking
    away. It was observed that children who waited till the adult came
    back and thus got two marshmallows seemed to also do well later in
    life including SAT tests etc.

    This test can also be interpreted as the child evaluating the
    chances of the adult coming back in a reasonable amount of time. If
    the child judges the adult to be unreliable (and has had prior
    experience with unreliable adults), he or she may just cut their
    losses early and eat the one marshmallow in front of them. This
    might even be rational if the return times of adults are governed
    by power laws.


** Randomness

   Sometimes, instead of figuring out the solution to a complex
   problem, a better strategy may be to sample it. For instance, if
   you want to know the probability of a shuffled deck being a
   winnable game in Solitaire, it may be best to play the game
   multiple times to approximate that probability.

   With these techniques, you won't know the result with certainty. In
   fact, while the earlier algorithms trade-off time and space
   (e.g. speed of computation versus cache size/memory size), the idea
   behind randomness is that certainty can also be a
   parameter to trade off.


** Game theory

   The Vickrey auction (the highest bidder gets the award but at the
   price of the second highest bid) awards honesty. In fact, honesty
   is a dominant strategy under this scheme which is the best result
   you can hope for.

   Myerson proved that any game in which players can
   use deception can be transformed into a game in which honesty is a
   dominant strategy. This is a surprising and uplifting result.


** Conclusion

   Type 1 rationality is the approach of grinding through all the
   available data and arriving at a decision. The stereotype is that
   computers are good at Type 1 rationality. However, in reality, even
   computers don't employ Type 1 rationality in anything beyond toy
   problem scale. In contrast, what is more useful is Type 2
   rationality i.e. finding a balanced approach of computational
   capability or burden, available memory and available data to come
   to a decision.

   This can lead to "computational kindness". For instance, when you
   meet friends for dinner, "where would you like to go?" is a
   question that transfers the computational burden to your
   friends. Instead, suggesting 3 options in order of your priority is
   not as "ethically polite" but may be computationally kinder.

   Algorithms such as Optimal Stopping only have a 37% chance of
   success. You can interpret this as a way of focusing on the process
   or the approach rather than the outcome. You may still fail in
   terms of the outcome but you can be comforted that you have adopted
   the optimal process.
