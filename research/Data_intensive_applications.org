#+TITLE: Designing data-intensive applications
#+Author: Martin Kleppmann
#+Filetags: :ComputerScience:BookNotes:Learning:

* Introduction

  In data-intensive applications, the storage, access
  and modification of data is the primary challenge as opposed to
  computation-intensive applications where the computations are the
  primary challenge.

  Data-intensive applications are systems comprising the following
  building blocks.
   - Databases :: to store data and access later
   - Caches :: to remember the results from expensive operations
   - Search indexes :: to search and filter data by keywords
   - Stream processing :: to send messages to asynchronous processes
   - Batch processing :: to periodically crunch a large amount of data

  Typically, some glue application code stitches together tools for
  each of these building blocks so that the resulting system is
  efficient. See [[file:Screenshot 2021-12-12 144728.jpg][this example]].

  Three concerns for such software systems are as follows.
    - Reliability :: The system must perform correctly even if some
      sub-parts work incorrectly (software or hardware faults or user
      errors).
    - Scalability :: As the system grows in user volume, data volume,
      complexity etc., the system should be able to handle this growth
      gracefully.
    - Maintainability :: As more people start using and developing the
      system (engineers and operations people), the system must be
      easily changeable so that people can work on it productively.

  Faults are defined as problems with sub-systems. Reliability is a
  measure of how much a fault in a sub-system results in a failure of
  the entire system.


* Relational versus document databases

  Relational databases are the traditional databases where each row is
  a tuple of information with potentially a unique id assigned to
  it. The collection of such tuples is a table. There may be multiple
  such tables. For instance, in a resume database, there may be a users
  table, an education table which lists the educational achievements
  of all the users in the users table, an experience table which lists
  all their work experience etc.

  Document databases (or noSQL databases) are like JSON or XML instances. In the same
  resume database example, all the information related to one user is
  stored in one JSON file as nodes. This leads to localization of
  information, which can be an advantage. The disadvantage is that
  many-to-one relationships are hard to represent e.g. all users who
  work at the same company. MongoDB is an example of a document database.

  Graph models can be thought of as another method of representing
  data. For instance, you may have graphs with vertices representing
  people, cities, states and countries. Edges between the vertices can
  have labels e.g. a city vertex may have a "within" edge to a state
  vertex. Similarly, a person vertex may have a "born in" edge or a "lives in"
  edge to a city vertex.

  A graph query can be of the type "find all
  people who live in the US". This query could traverse an unknown
  number of vertices since some people may have a full address while
  some may only have the name of the city or state they live
  in. This is an advantage of a graph query over a relational database
  query.


* Query languages

  SQL is a declarative language i.e. it follows relational algebra
  very closely. Consider a database for animals where some of the
  animals may be of the shark family.

  In relational algebra, this is represented as:
  \[ Sharks = \Sigma_{family = "Sharks"} Animals \]

  In SQL, this representation is followed closely as:
  #+begin_src
  SELECT * FROM animals WHERE family = "sharks"
  #+end_src

  This doesn't guarantee any particular order, etc., so the query
  optimizer can choose how to implement this query. In this sense,
  declarative languages are different from imperative languages that
  tell a compiler/optimizer *how* to generate some data.

  In addition, declarative queries can be more concise and may lend
  themselves well to parallelization.

  Many DB languages allow MapReduce extensions. The Map part is a
  function that runs on each selected item in a database and emits a
  (key, value) pair. The Reduce part then takes all such tuples that
  have the same key value and runs a function to calculate an
  aggregate value (such as the sum of all the values). MapReduce
  functions have certain restrictions e.g. they must be pure functions
  and must have no side effects. With these restrictions, they allow
  for significant parallelism. Hadoop MapReduce allows for Java
  callback functions for the map() and reduce() parts. In between, the
  MapReduce intrinsics automatically do merging and sorting. The
  inputs are typically files in the Hadoop Distributed File System (HDFS).

  [[file:Screenshot 2022-01-09 114730.jpg][Pictorial representation of MapReduce]]


* Under the hood of databases

  A log file in which new (key, value) pairs are appended could
  act as a simple database. To get a value, you just search in the
  file till you find the value corresponding to the key. The set()
  function is quite efficient since it just involves writing something
  at the end of a file. The get() however is inefficient since it
  takes O(n) operations where n is the number of log entries.

  An enhancement is to create a hash table in memory that
  maintains a byte offset for each key. When a read request comes in,
  you look up the byte offset for the key in the hash table and go to
  that file offset and read the data. Also, the data in the disk
  should be stored in binary format for storage optimization and not
  text format.

  The log file is only appended-to and no updates are
  done in-place. This avoids random access writes which can be
  costly. Appending is done through a single writer thread to avoid
  concurrency issues.

  An enhancement from a simple hash table is to use a sorted segment
  table (SST). In this system, the keys are all sorted. The hash table
  now does not need to contain the byte offsets for all keys but only
  for a sparse subset of the keys.


* Online Analytics Processing (OLAP)

  Previously, standard queries were all of the type where one or a few
  records from a database was accessed and one or all of the fields
  were presented to the end-user. This was called Online Transaction
  Processing (OLTP). More recently, business analysts need to run
  queries where a large number of records are accessed and summary
  statistics calculations are run on them. These are called OLAP.

  Typically, you don't want OLAP queries running on the same servers
  that run your production databases. Instead, a new concept of data
  warehouses has emerged where data is occasionally synced from the
  transaction servers to the data warehouse. The data warehouse is
  structured to efficiently run analytical queriees. Input data is fed
  into the warehouse through an Extract-Transform-Load (ETL) mechanism
  where the data from several transaction databases is extracted,
  cleaned up for querying and uploaded to the warehouse.

  Most data warehouses are relational databases and use a standard
  star schema. Under this schema, there is a central "fact table" in
  which each record is an event of a transaction/web click etc. Each
  record then has links to records in other "dimension tables" such as
  customer tables, store tables, product tables etc. to provide more
  details on the what/how/when of the event. Large data warehouses
  could have trillions of rows in their fact tables. Each row in a fact
  table could have hundreds of columns.

  The typical data query requires only a few of those hundreds of
  columns to be returned for a huge number of records. The performance
  of such queries have to be optimized in data warehouses. This is
  different from a typical query in a transaction database.

  In order to support this, column-wise storage can be used where each
  column is stored separately. When a query requires the results from
  some columns, only those columns need to be accessed. Further,
  column-wise storage can make compression of data easy since the data
  in each column may not vary much. Finally, techniques like bit-map
  encoding can be used to improve performance. In bit-map encoding, if
  entries in a column can take on a small subset of values, then a bit
  map is created for each of the values that can be taken on. The ith
  bit map value indicates whether row i contains that value or not.


* Links

  - [[https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable-ebook-dp-B06XPJML5D/dp/B06XPJML5D/ref=mt_other?_encoding=UTF8&me=&qid=][Amazon link]]
  - [[https://github.com/ept/ddia-references][Updated links to references]]
