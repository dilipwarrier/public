#+TITLE: Apache projects
#+FILETAGS: :Software:Data:
#+STARTUP: contents, hideallblocks

* Apache Spark
:PROPERTIES:
:ID:       be4c0342-3c8d-49ac-8046-f36b65361d72
:END:

See [[id:c63978d7-17dd-49e9-a52f-2fb47c7190c3][What is Apache Spark?]]

* Apache Cassandra

Distributed, highly scalable NoSQL database

Optimized for OLTP (online transactional processing):

High write throughput

Low-latency reads

Time-series and event workloads

Always-on availability

Rows stored in SSTables on disk

Queryed via CQL (Cassandra Query Language)

Used when you need fast, operational data access.

Examples:

IoT event ingestion

Time-series dashboards

User profile storage

Sensor data

Fraud scoring systems

* Apache Iceberg

A table format that sits on top of a data lake (S3, HDFS, GCS, Azure Blob)

Not a database - it tracks structured tables stored as Parquet/ORC files

Built for OLAP (analytical workloads):

Large-scale scans

ACID transactions in the data lake

Schema evolution

Partitioning

Time travel

Versioning

Queried via engines like Spark, Trino, Flink, Dremio, Snowflake (Iceberg tables)

Used when you need large-scale analytics on massive datasets.

Examples:

ML feature stores

Batch analytics

ETL pipelines

BI dashboards

Data warehouses built on S3 ("lakehouses")

* Apache Airflow

Apache Airflow is an open-source platform for developing, scheduling,
and monitoring batch-oriented workflows. It generates a Directed Acyclic
Graph (DAG) to specify a workflow. It is written in pure Python.

Kubernetes takes care of executing each of the tasks specified by Airflow.

** References

- [[https://airflow.apache.org/docs/apache-airflow/stable/index.html][Airflow documentation site]]

* Apache and CNCF
:PROPERTIES:
:ID:       c9c9b3aa-bd10-46eb-9c7c-f804bbefc94a
:END:

CNCF addresses runtime infrastructure. It answers the question: How
do we run and observe cloud applications safely, reliably, and portably?

Apache addresses data and compute frameworks. It answers the question:
How do we move, transform, and analyze data efficiently and scalably?

You can think of CNCF tools as horizontal enablers, and Apache tools
as vertical data capabilities. In cloud-native data platforms, you
often run Apache tools on CNCF infrastructure.

|---------------------------+------------------------------------------+----------------------------------+---------------------|
| Layer                     | Representative Technologies / Tools      | Notes / Ecosystem                | Open-source project |
|---------------------------+------------------------------------------+----------------------------------+---------------------|
| Business intelligence     | Superset, Metabase, Grafana, Redash      | Visualization and BI dashboards  | Apache              |
| Query / analytics         | Druid, Hive, Trino                       | OLAP engines and query layers    | Apache              |
| Data processing           | Spark, Flink, Beam, Airflow              | ETL and stream/batch computation | Apache              |
| Data ingestion            | Kafka, NiFi, Pulsar, Debezium            | Data capture and event streams   | Apache              |
| Data storage              | Hudi, Iceberg, Parquet, HDFS             | Data lakes and table formats     | Apache              |
| Infrastructure            | Kubernetes, Helm, Argo, Prometheus, etc. | Ecosystem for orchestration      | CNCF                |
| Cloud provider / hardware | AWS, GCP, Azure, Bare-metal              | Underlying compute and storage   |                     |
|---------------------------+------------------------------------------+----------------------------------+---------------------|
